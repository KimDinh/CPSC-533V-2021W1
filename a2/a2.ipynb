{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPSC 533V: Assignment 2 - Tabular Q Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 1.2em;\">Due Date: Wed Oct 6, 2021</p>\n",
    "<p style=\"font-size: 1.2em;\">100 Points Total (9% of final grade)</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Notes\n",
    "\n",
    "* Your deliverable is this Jupyter Notebook. Submission will be done via Canvas.\n",
    "* For instructions on installing and running Jupyter Notebook: https://jupyter.org/install\n",
    "    * Start by cloning this repository: git clone git@github.com:UBCMOCCA/CPSC533V_2021W1.git\n",
    "    * Install Jupyter Notebook using either `conda install jupyter` or `pip install jupyter`\n",
    "    * Inside the `a2` folder, run `jupyter notebook` and a webpage should open in the browser\n",
    "    * If not, follow the instruction in terminal to launch an interactive session\n",
    "* If you use additional Python packages, please list them  as it will help with grading. \n",
    "* **We recommend working in groups of two**. List your names and student numbers below (if you use a different name on Canvas).\n",
    "\n",
    "<ul style=\"list-style-type: none; font-size: 1.2em;\">\n",
    "<li>Name (and student ID): Kim Dinh </li>\n",
    "<li>Name (and student ID): Alan Milligan </li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "## Debugging Tips\n",
    "\n",
    "* Debugging in Jupyter Notebook can be using `pdb` or `ipdb`\n",
    "* Insert `import ipdb; ipdb.set_trace()` to where you want to set a breakpoint\n",
    "* See https://docs.python.org/3/library/pdb.html#debugger-commands for useful commands\n",
    "* Remember to quit out from an `ipdb` session, otherwise you may wonder why a code cell is taking forever to complete ðŸ˜‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##  Tabular Q-Learning\n",
    "\n",
    "Tabular Q-learning is an RL algorithm for problems with discrete states and discrete actions. The algorithm is described in the class notes, which borrows the summary description from [Section 6.5](http://incompleteideas.net/book/RLbook2018.pdf#page=153) of Richard Sutton's RL book. In the tabular approach, the Q-value is represented as a lookup table. As discussed in class, Q-learning can further be extended to continuous states and discrete actions, leading to the [Atari DQN](https://arxiv.org/abs/1312.5602) / Deep Q-learning algorithm.  However, it is important and informative to first fully understand tabular Q-learning.\n",
    "\n",
    "Informally, Q-learning works as follows: The goal is to learn the optimal Q-function: \n",
    "`Q(s,a)`, which is the *value* of being at state `s` and taking action `a`.  Q tells you how well you expect to do, on average, from here on out, given that you act optimally.  Once the Q function is learned, choosing an optimal action is as simple as looping over all possible actions and choosing the one with the highest Q (optimal action $a^* = \\text{max}_a Q(s,a)$).  To learn Q, we initialize it arbitrarily and then iteratively refine it using the Bellman backup equation for Q functions, namely: \n",
    "$Q(s,a) \\leftarrow Q(s,a) + \\alpha [r + \\gamma \\text{max}_a Q(s', a) - Q(s,a)]$.\n",
    "Here, $r$ is the reward associated with with the transition from state s to s', and $\\alpha$ is a learning rate.\n",
    "\n",
    "In this assignment you will implement tabular Q-learning and apply it to CartPole â€“ an environment with a **continuous** state space.  To apply the tabular method, **you will need to discretize the CartPole state space** by dividing the state-space into bins.\n",
    "\n",
    "\n",
    "**Assignment goals:**\n",
    "- To become familiar with Python, NumPy, and OpenAI Gym\n",
    "- To understand and implement tabular Q-learning\n",
    "- To experiment tabular Q-learning on your implemention of discrete CartPole environment\n",
    "- (Optional) To develop further intuition regarding possible variations of the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Deep reinforcement learning has generated impressive results for board games ([Go][go], [Chess/Shogi][chess]), video games ([Atari][atari], , [DOTA2][dota], [StarCraft II][scii]), [and][baoding] [robotic][rubix] [control][anymal] ([of][cassie] [course][mimic] ðŸ˜‰).  RL is beginning to work for an increasing range of tasks and capabilities.  At the same time, there are many [gaping holes][irpan] and [difficulties][amid] in applying these methods. Understanding deep RL is important if you wish to have a good grasp of the modern landscape of control methods.\n",
    "\n",
    "These next several assignments are designed to get you started with deep reinforcement learning, to give you a more close and personal understanding of the methods, and to provide you with a good starting point from which you can branch out into topics of interest. You will implement basic versions of some of the important fundamental algorithms in this space, including Q-learning, policy gradient, and search methods.\n",
    "\n",
    "We will only have time to cover a subset of methods and ideas in this space.\n",
    "If you want to dig deeper, we suggest following the links given on the course webpage.  Additionally we draw special attention to the [Sutton book](http://incompleteideas.net/book/RLbook2018.pdf) for RL fundamentals and in depth coverage, and OpenAI's [Spinning Up resources](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html) for a concise intro to RL and deep RL concepts, as well as good comparisons and implementations of modern deep RL algorithms.\n",
    "\n",
    "\n",
    "[atari]: https://arxiv.org/abs/1312.5602\n",
    "[go]: https://deepmind.com/research/case-studies/alphago-the-story-so-far\n",
    "[chess]:https://deepmind.com/blog/article/alphazero-shedding-new-light-grand-games-chess-shogi-and-go \n",
    "[dota]: https://openai.com/blog/openai-five/\n",
    "[scii]: https://deepmind.com/blog/article/AlphaStar-Grandmaster-level-in-StarCraft-II-using-multi-agent-reinforcement-learning\n",
    "[baoding]: https://bair.berkeley.edu/blog/2019/09/30/deep-dynamics/\n",
    "[rubix]: https://openai.com/blog/solving-rubiks-cube/\n",
    "[cassie]: https://www.cs.ubc.ca/~van/papers/2019-CORL-cassie/index.html\n",
    "[mimic]: https://www.cs.ubc.ca/~van/papers/2018-TOG-deepMimic/index.html\n",
    "[anymal]: https://arxiv.org/abs/1901.08652\n",
    "\n",
    "\n",
    "[irpan]: https://www.alexirpan.com/2018/02/14/rl-hard.html\n",
    "[amid]: http://amid.fish/reproducing-deep-rl\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "# Only run if necessary\n",
    "#!pip install gym\n",
    "#!pip install numpy\n",
    "#!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import itertools\n",
    "import numpy as np\n",
    "import gym\n",
    "import copy, math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Explore the CartPole environment [18 pts]\n",
    "\n",
    "Your first task is to familiarize yourself with the OpenAI gym interface and the [CartPole environment]( https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py)\n",
    "by writing a simple hand-coded policy to try to solve it.  \n",
    "Read this brief introduction on [OpenAI Gym](https://gym.openai.com/docs/) to get started. \n",
    "The gym interface is very popular and you will see many algorithm implementations and \n",
    "custom environments that support it.  You may even want to use the API in your course projects, \n",
    "to define a custom environment for a task you want to solve.\n",
    "\n",
    "Below is some example code that runs a simple random policy.  You are to:\n",
    "- **run the code to see what it does**\n",
    "- **write code that chooses an action based on the observation**.  You will need to learn about the gym API and to read the CartPole documentation to figure out what the `action` and `obs` vectors mean for this environment. \n",
    "Your hand-coded policy can be arbitrary, and it should ideally do better than the random policy.  There is no single correct answer. The goal is to become familiar with `env`s.\n",
    "- **write code to print out the total reward gained by your policy in a single episode run**\n",
    "- **answer the short-response questions below** (see the TODOs for all of this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation space: Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n",
      "action space: Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')  # you can also try LunarLander-v2, but make sure to change it back\n",
    "print('observation space:', env.observation_space)\n",
    "print('action space:', env.action_space)\n",
    "\n",
    "# To find out what the observations mean, read the CartPole documentation.\n",
    "# Uncomment the lines below, or visit the source file: \n",
    "# https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py\n",
    "\n",
    "#cartpole = env.unwrapped\n",
    "#cartpole?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1 [10pts] Complete the `TODO`s in the next code block**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward: 39.00\n"
     ]
    }
   ],
   "source": [
    "# Q1.1\n",
    "\n",
    "# Runs a single episode and render it\n",
    "# Try running this before editing anything\n",
    "\n",
    "obs = env.reset()  # get initial state/observation\n",
    "total_reward = 0\n",
    "\n",
    "while True:\n",
    "    # TODO: replace this `action` with something that depends on `obs`\n",
    "    action = 0 if obs[2] < 0 else 1\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    \n",
    "    env.render()\n",
    "    time.sleep(0.1)  # so it doesn't render too quickly\n",
    "    if done: break\n",
    "env.close()\n",
    "\n",
    "# TODO: print out your total sum of rewards here\n",
    "print(\"Total reward: {:.2f}\".format(total_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2. [2pts] Describe the observation and action spaces of CartPole.  What does each of the values mean/do?**\n",
    "\n",
    "*Hint: Look at the full [source code here](https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py) if you haven't already.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observation space of CartPole is a subset of $\\mathbb{R}^4$ and specifies the possible values of the state. The range of each dimension of observation space indicate the range of cart position, cart velocity, pole angle, pole angular velocity respectively.\n",
    "\n",
    "The action space of CartPole is a discrete space with 2 elements: \"move left\" and \"move right\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3. [2pts] What distribution is used to sample initial states? (see the `reset` function)** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intial states are sampled from the uniform distribution on $[-0.05, 0.05]^4$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.4. [2pts] What is the termination condition, which determines if the `env` is `done`?** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The termination condition is that the absolute value of either cart position or pole angle exceeds the thresholds given by ```self.x_threshold``` and ```self.theta_threshold_radians``` in the source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.5. [2pts] Briefly describe your policy.  What observation information does it use?  What score did you achieve (rough maximum and average)?  And how does it compare to the random policy?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My policy is that if the pole is leaned to the left (negative pole angle) then push left, otherwise push right. After 10 simulations, the maximum and average total reward is 55 and 44.2 respectively. This is clearly better than the random policy which has average total reward 21.9 after 10 simulations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Discretize the environment [32 pts]\n",
    "\n",
    "Next, we need to discretize CartPole's continuous state space to work for tabular Q-learning.  While this is in part  a contrived usage of tabular methods, given the existence of other approaches that are designed to cope with continuous state-spaces, it is also interesting to consider whether tabular methods can be adapted more directly via discretization of the state into bins. Furthermore, tabular methods are simple, interpretabile, and can be proved to converge, and thus they still remain relevant.\n",
    "\n",
    "Your task is to discretize the state/observation space so that it is compatible with tabular Q-learning.  To do this:\n",
    "- **implement `obs_normalizer` to pass its test**\n",
    "- **implement `get_bins` to pass its test**\n",
    "- **then answer questions 2.3 and 2.4**\n",
    "\n",
    "[map]: https://arxiv.org/abs/1504.04909\n",
    "[qd]: https://quality-diversity.github.io/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1 [15pts for passing test_normed]** Normalize observation space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2.1\n",
    "\n",
    "def obs_normalizer(obs):\n",
    "    \"\"\"Normalize the observations between 0 and 1\n",
    "    \n",
    "    If the observation has extremely large bounds, then clip to a reasonable range before normalizing; \n",
    "    (-2,2) should work.  (It is ok if the solution is specific to CartPole)\n",
    "    \n",
    "    Args:\n",
    "        obs (np.ndarray): shape (4,) containing an observation from CartPole using the bound of the env\n",
    "    Returns:\n",
    "        normed (np.ndarray): shape (4,) where all elements are roughly uniformly mapped to the range [0, 1]\n",
    "    \n",
    "    \"\"\"\n",
    "    # HINT: check out env.observation_space.high, env.observation_space.low\n",
    "    \n",
    "    # TODO: implement this function\n",
    "    normed = copy.deepcopy(obs)\n",
    "    x_high, x_low = 4.8, -4.8\n",
    "    theta_high, theta_low = 48 * math.pi / 360, -48 * math.pi / 360\n",
    "    \n",
    "    # clip cart velocity and pole angular velocity to [-2,2]\n",
    "    normed[[1,3]] = np.clip(normed[[1,3]], -2, 2)\n",
    "    # normalize\n",
    "    normed[0] = (normed[0] - x_low) / (x_high - x_low)\n",
    "    normed[2] = (normed[2] - theta_low) / (theta_high - theta_low)\n",
    "    normed[[1,3]] = (normed[[1,3]] + 2) / 4\n",
    "    \n",
    "    return normed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed!\n"
     ]
    }
   ],
   "source": [
    "### TEST 2.1\n",
    "def test_normed():\n",
    "    obs = env.reset()\n",
    "    while True:\n",
    "        obs, _, done, _ =  env.step(env.action_space.sample())\n",
    "        normed = obs_normalizer(obs) \n",
    "        assert np.all(normed >= 0.0) and np.all(normed <= 1.0), '{} are outside of (0,1)'.format(normed)\n",
    "        if done: break\n",
    "    env.close()\n",
    "    print('Passed!')\n",
    "test_normed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2 [13pts for passing test_binned]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2.2\n",
    "\n",
    "def get_bins(normed, num_bins):\n",
    "    \"\"\"Map normalized observations (0,1) to bin index values (0,num_bins-1)\n",
    "    \n",
    "    Args:\n",
    "        normed (np.ndarray): shape (4,) output from obs_normalizer\n",
    "        num_bins (int): how many bins to use\n",
    "    Returns:\n",
    "        binned (np.ndarray of type np.int): shape (4,) where all elements are values in range [0,num_bins-1]\n",
    "    \n",
    "    \"\"\"\n",
    "    # TODO: implement this function\n",
    "    # take minimum with num_bins - 1 so that the value 1 is in the last bin\n",
    "    binned = np.minimum((normed * num_bins).astype(np.int32), num_bins - 1)\n",
    "    \n",
    "    return binned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed!\n"
     ]
    }
   ],
   "source": [
    "### TEST 2.2\n",
    "obs = env.reset()\n",
    "env.close()\n",
    "\n",
    "def test_binned(num_bins):\n",
    "    normed = np.array([0.0, 0.2, 0.8, 1.0])\n",
    "    binned = get_bins(normed, num_bins)\n",
    "    assert np.all(binned >= 0) and np.all(binned < num_bins), '{} supposed to be between (0, {})'.format(binned, num_bins-1)\n",
    "    assert binned.dtype == np.int32, \"You should also make sure to cast your answer to int using np.int() or arr.astype(np.int)\" \n",
    "    \n",
    "test_binned(5)\n",
    "test_binned(10)\n",
    "test_binned(50)\n",
    "print('Passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3. [2pts] If your state has 4 values and each is binned into N possible bins, how many bins are needed to represent all unique possible states)?**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$N^4$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.4. [2pts] After discretizing state space, is the dynamics deterministic or non-deterministic? Explain your answer in one to two sentences.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The discretization causes the dynamics to become stochastic. This is because we lose information when we go from continuous values to bins, so this problems turned from an MDP to a POMDP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Solve the env [30 pts] \n",
    "\n",
    "Using the pseudocode below and the functions you implemented above, implement tabular Q-learning and use it to solve CartPole.\n",
    "\n",
    "We provide setup code to initialize the Q-table and give examples of interfacing with it. Write the inner and outer loops to train your algorithm.  These training loops will be similar to those deep RL approaches, so get used to writing them!\n",
    "\n",
    "The algorithm (excerpted from Section 6.5 of [Sutton's book](http://incompleteideas.net/book/RLbook2018.pdf)) is given below:\n",
    "\n",
    "![Sutton RL](https://i.imgur.com/mdcWVRL.png)\n",
    "\n",
    "in summary:\n",
    "- **implement Q-learning using this pseudocode and the helper code**\n",
    "- **answer the questions below**\n",
    "- **run the suggested experiments and otherwise experiment with whatever interests you**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Q Table:  (10, 10, 10, 10, 2)\n",
      "Original obs [ 0.01712155  0.01500509 -0.00751451  0.0068919 ] --> binned (5, 5, 4, 5)\n",
      "Value of Q Table at that obs/state value [0. 0.]\n",
      "(2,)\n"
     ]
    }
   ],
   "source": [
    "# setup (see last few lines for how to use the Q-table)\n",
    "\n",
    "# hyper parameters. feel free to change these as desired and experiment with different values\n",
    "num_bins = 10\n",
    "alpha = 0.1\n",
    "gamma = 0.99\n",
    "log_n = 1000\n",
    "# epsilon greedy\n",
    "eps = 0.05  #usage: action = optimal if np.random.rand() > eps else random\n",
    "\n",
    "obs = env.reset()\n",
    "\n",
    "# Q-table initialized to zeros.  first 4 dims are state, last dim is for action (0,1) for left,right.\n",
    "Q = np.zeros([num_bins]*len(obs)+[env.action_space.n])\n",
    "\n",
    "# helper function to convert observation into a binned state so we can index into our Q-table\n",
    "obs2bin = lambda obs: tuple(get_bins(obs_normalizer(obs), num_bins=num_bins))\n",
    "\n",
    "s = obs2bin(obs)\n",
    "\n",
    "print('Shape of Q Table: ', Q.shape) # you can imagine why tabular learning does not scale very well\n",
    "print('Original obs {} --> binned {}'.format(obs, s))\n",
    "print('Value of Q Table at that obs/state value', Q[s])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1 [25pts] Implement Q-learning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3.1\n",
    "\n",
    "# TODO: implement Q learning, following the pseudo-code above. \n",
    "#     - you can follow it almost exactly, but translating things for the gym api and our code used above\n",
    "#     - make sure to use e-greedy, where e = random about 0.05 percent of the time\n",
    "#     - make sure to do the S <-- S' step because it can be easy to forget\n",
    "#     - every log_n steps, you should render your environment and\n",
    "#       print out the average total episode rewards of the past log_n runs to monitor how your agent trains\n",
    "#      (your implementation should be able to break at least +150 average reward value, and you can use that \n",
    "#       as a breaking condition.  It make take several minutes to run depending on your computer.)\n",
    "\n",
    "# if average_reward_goal is None then train with exactly num_episodes episodes\n",
    "# otherwise train until either average reward over the log_n episodes exceed average_reward_goal\n",
    "def CartPole_Q_learning(num_episodes=20000, num_bins=10, alpha=0.1, gamma=0.99, \n",
    "                        log_n=1000, eps=0.05, average_reward_goal=150):\n",
    "    # Q-table initialized to zeros.\n",
    "    Q = np.zeros([num_bins]*len(env.reset())+[env.action_space.n])\n",
    "    \n",
    "    # episode_rewards stores the total rewards of each episode\n",
    "    episode_rewards = []\n",
    "    \n",
    "    for episode in range(num_episodes):   \n",
    "        S = obs2bin(env.reset())\n",
    "        episode_rewards.append(0)     \n",
    "        while True:            \n",
    "            # choose action by epsilon-greedy policy\n",
    "            A = np.argmax(Q[S]) if np.random.rand() > eps else np.random.choice(2)\n",
    "            \n",
    "            # take action, update Q and S\n",
    "            obs, R, done, _ = env.step(A)\n",
    "            S_next = obs2bin(obs)\n",
    "            Q[S][A] = Q[S][A] + alpha * (R + gamma * np.max(Q[S_next]) - Q[S][A])\n",
    "            S = S_next\n",
    "            episode_rewards[episode] += R\n",
    "            '''\n",
    "            # render after every log_n episodes\n",
    "            if (episode + 1) % log_n == 0: \n",
    "                env.render()\n",
    "                time.sleep(0.01)\n",
    "            '''\n",
    "            if done: break\n",
    "        \n",
    "        # print out average total episode rewards after every log_n episodes\n",
    "        if (episode + 1) % log_n == 0:\n",
    "            average_reward = np.mean(episode_rewards[-log_n:])\n",
    "            print(\"Average rewards of the past {} episodes at iteration {}: {:.2f}\".format(log_n, episode+1, average_reward))\n",
    "            if (average_reward_goal is not None) and (average_reward > average_reward_goal): break\n",
    "\n",
    "    if (average_reward_goal is not None) and (np.mean(episode_rewards[-log_n:]) <= average_reward_goal):\n",
    "        print(\"WARNING: Average reward did not exceed average_reward_goal\")\n",
    "\n",
    "    return Q, episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average rewards of the past 1000 episodes at iteration 1000: 10.98\n",
      "Average rewards of the past 1000 episodes at iteration 2000: 10.91\n",
      "Average rewards of the past 1000 episodes at iteration 3000: 11.19\n",
      "Average rewards of the past 1000 episodes at iteration 4000: 10.68\n",
      "Average rewards of the past 1000 episodes at iteration 5000: 10.82\n",
      "Average rewards of the past 1000 episodes at iteration 6000: 28.96\n",
      "Average rewards of the past 1000 episodes at iteration 7000: 71.94\n",
      "Average rewards of the past 1000 episodes at iteration 8000: 133.59\n",
      "Average rewards of the past 1000 episodes at iteration 9000: 214.16\n"
     ]
    }
   ],
   "source": [
    "_, _ = CartPole_Q_learning()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2 [5pts] Plot the learning curve**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average rewards of the past 1000 episodes at iteration 1000: 10.47\n",
      "Average rewards of the past 1000 episodes at iteration 2000: 10.59\n",
      "Average rewards of the past 1000 episodes at iteration 3000: 10.55\n",
      "Average rewards of the past 1000 episodes at iteration 4000: 10.98\n",
      "Average rewards of the past 1000 episodes at iteration 5000: 19.59\n",
      "Average rewards of the past 1000 episodes at iteration 6000: 71.21\n",
      "Average rewards of the past 1000 episodes at iteration 7000: 177.68\n",
      "Average rewards of the past 1000 episodes at iteration 8000: 230.92\n",
      "Average rewards of the past 1000 episodes at iteration 9000: 287.71\n",
      "Average rewards of the past 1000 episodes at iteration 10000: 249.22\n",
      "Average rewards of the past 1000 episodes at iteration 1000: 9.69\n",
      "Average rewards of the past 1000 episodes at iteration 2000: 9.88\n",
      "Average rewards of the past 1000 episodes at iteration 3000: 9.81\n",
      "Average rewards of the past 1000 episodes at iteration 4000: 9.83\n",
      "Average rewards of the past 1000 episodes at iteration 5000: 9.81\n",
      "Average rewards of the past 1000 episodes at iteration 6000: 9.97\n",
      "Average rewards of the past 1000 episodes at iteration 7000: 37.25\n",
      "Average rewards of the past 1000 episodes at iteration 8000: 119.25\n",
      "Average rewards of the past 1000 episodes at iteration 9000: 212.09\n",
      "Average rewards of the past 1000 episodes at iteration 10000: 231.48\n",
      "Average rewards of the past 1000 episodes at iteration 1000: 9.61\n",
      "Average rewards of the past 1000 episodes at iteration 2000: 9.85\n",
      "Average rewards of the past 1000 episodes at iteration 3000: 9.86\n",
      "Average rewards of the past 1000 episodes at iteration 4000: 9.91\n",
      "Average rewards of the past 1000 episodes at iteration 5000: 9.91\n",
      "Average rewards of the past 1000 episodes at iteration 6000: 28.13\n",
      "Average rewards of the past 1000 episodes at iteration 7000: 121.92\n",
      "Average rewards of the past 1000 episodes at iteration 8000: 205.72\n",
      "Average rewards of the past 1000 episodes at iteration 9000: 245.37\n",
      "Average rewards of the past 1000 episodes at iteration 10000: 262.35\n",
      "Average rewards of the past 1000 episodes at iteration 1000: 10.20\n",
      "Average rewards of the past 1000 episodes at iteration 2000: 10.35\n",
      "Average rewards of the past 1000 episodes at iteration 3000: 10.40\n",
      "Average rewards of the past 1000 episodes at iteration 4000: 10.41\n",
      "Average rewards of the past 1000 episodes at iteration 5000: 10.50\n",
      "Average rewards of the past 1000 episodes at iteration 6000: 12.77\n",
      "Average rewards of the past 1000 episodes at iteration 7000: 138.28\n",
      "Average rewards of the past 1000 episodes at iteration 8000: 264.98\n",
      "Average rewards of the past 1000 episodes at iteration 9000: 260.24\n",
      "Average rewards of the past 1000 episodes at iteration 10000: 197.35\n",
      "Average rewards of the past 1000 episodes at iteration 1000: 9.64\n",
      "Average rewards of the past 1000 episodes at iteration 2000: 9.84\n",
      "Average rewards of the past 1000 episodes at iteration 3000: 9.86\n",
      "Average rewards of the past 1000 episodes at iteration 4000: 9.92\n",
      "Average rewards of the past 1000 episodes at iteration 5000: 9.81\n",
      "Average rewards of the past 1000 episodes at iteration 6000: 27.56\n",
      "Average rewards of the past 1000 episodes at iteration 7000: 107.29\n",
      "Average rewards of the past 1000 episodes at iteration 8000: 160.31\n",
      "Average rewards of the past 1000 episodes at iteration 9000: 255.02\n",
      "Average rewards of the past 1000 episodes at iteration 10000: 278.71\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PolyCollection at 0x120f1edf0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Q3.2\n",
    "\n",
    "# TODO: Plot the learning curve.\n",
    "#       Below is a snippet for generate a curve with upper and lower bounds.\n",
    "#       From your training loop above, save the episode rewards.\n",
    "#       Rerun the training code a few times to get min and max.\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from bottleneck import move_mean\n",
    "\n",
    "num_episodes = 10000\n",
    "\n",
    "# lists of min and max episode rewards\n",
    "min_episode_rewards = np.full(num_episodes, np.inf)\n",
    "max_episode_rewards = np.full(num_episodes, 0)\n",
    "mean_episode_rewards = np.full(num_episodes, 0)\n",
    "\n",
    "plt.close()\n",
    "for i in range(5):\n",
    "    # To make the plotting easier, set average_reward_goal to None and run with same num_episodes\n",
    "    # (see Cartpole_Q_learning implementation above)\n",
    "    _, episode_rewards = CartPole_Q_learning(num_episodes=num_episodes, average_reward_goal=None)\n",
    "\n",
    "    min_episode_rewards = np.minimum(min_episode_rewards, episode_rewards)\n",
    "    max_episode_rewards = np.maximum(max_episode_rewards, episode_rewards)\n",
    "    mean_episode_rewards = mean_episode_rewards + np.array(episode_rewards)/5\n",
    "\n",
    "    #plt.plot(episode_rewards)\n",
    "\n",
    "window_size = 100\n",
    "plt.plot(move_mean(mean_episode_rewards, window_size))\n",
    "plt.fill_between(np.arange(num_episodes),\n",
    "                move_mean(min_episode_rewards, window_size),\n",
    "                move_mean(max_episode_rewards, window_size),\n",
    "                alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Experiments [20 pts]\n",
    "\n",
    "Given a working algorithm, you will run a few experiments.  Either make a copy of your code above to modify, or make the modifications in a way that they can be commented out or switched between (with boolean flag if statements)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.2. [10pts] $\\epsilon$-greedy.**  How sensitive are the results to the value of $\\epsilon$?   First, write down your prediction of what would happen if $\\epsilon$ is set to various values, including for example [0, 0.05, 0.25, 0.5]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At $\\epsilon = 0$, we will never learn as there are no explore actions taken, and since we initlized to 0 we are just going to go left and die each time. For 0.05, this is what we did above so I \"predict\" that it starts off somewhat low but tranins relativly quickly. For 0.25, I suspect the peak performance will be somewhat worse than 0.05, but the rate at which it gets there to be much faster due to better explore near the start. For 0.5, I suspect peak performance will be pretty bad given no matter how good of a policy you learn, you will be taking random actions half the time and so there is a pretty good chance you'll do an unlucky sequence and die."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the experiment and observe the impact on the algorithm.  Report the results below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average rewards of the past 1000 episodes at iteration 1000: 9.32\n",
      "Average rewards of the past 1000 episodes at iteration 2000: 9.35\n",
      "Average rewards of the past 1000 episodes at iteration 3000: 9.36\n",
      "Average rewards of the past 1000 episodes at iteration 4000: 9.34\n",
      "Average rewards of the past 1000 episodes at iteration 5000: 9.34\n",
      "Average rewards of the past 1000 episodes at iteration 6000: 9.32\n",
      "Average rewards of the past 1000 episodes at iteration 7000: 9.37\n",
      "Average rewards of the past 1000 episodes at iteration 8000: 9.38\n",
      "Average rewards of the past 1000 episodes at iteration 9000: 9.35\n",
      "Average rewards of the past 1000 episodes at iteration 10000: 9.34\n",
      "Average rewards of the past 1000 episodes at iteration 11000: 9.32\n",
      "Average rewards of the past 1000 episodes at iteration 12000: 9.32\n",
      "Average rewards of the past 1000 episodes at iteration 13000: 9.32\n",
      "Average rewards of the past 1000 episodes at iteration 14000: 9.34\n",
      "Average rewards of the past 1000 episodes at iteration 15000: 9.33\n",
      "Average rewards of the past 1000 episodes at iteration 16000: 9.34\n",
      "Average rewards of the past 1000 episodes at iteration 17000: 9.37\n",
      "Average rewards of the past 1000 episodes at iteration 18000: 9.37\n",
      "Average rewards of the past 1000 episodes at iteration 19000: 9.35\n",
      "Average rewards of the past 1000 episodes at iteration 20000: 9.30\n",
      "WARNING: Average reward did not exceed average_reward_goal\n"
     ]
    }
   ],
   "source": [
    "_, _ = CartPole_Q_learning(eps=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average rewards of the past 1000 episodes at iteration 1000: 9.74\n",
      "Average rewards of the past 1000 episodes at iteration 2000: 9.84\n",
      "Average rewards of the past 1000 episodes at iteration 3000: 9.96\n",
      "Average rewards of the past 1000 episodes at iteration 4000: 9.89\n",
      "Average rewards of the past 1000 episodes at iteration 5000: 16.43\n",
      "Average rewards of the past 1000 episodes at iteration 6000: 89.72\n",
      "Average rewards of the past 1000 episodes at iteration 7000: 175.26\n"
     ]
    }
   ],
   "source": [
    "_, _ = CartPole_Q_learning(eps=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average rewards of the past 1000 episodes at iteration 1000: 43.15\n",
      "Average rewards of the past 1000 episodes at iteration 2000: 128.95\n",
      "Average rewards of the past 1000 episodes at iteration 3000: 167.55\n"
     ]
    }
   ],
   "source": [
    "_, _ = CartPole_Q_learning(eps=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average rewards of the past 1000 episodes at iteration 1000: 41.18\n",
      "Average rewards of the past 1000 episodes at iteration 2000: 80.14\n",
      "Average rewards of the past 1000 episodes at iteration 3000: 88.22\n",
      "Average rewards of the past 1000 episodes at iteration 4000: 88.66\n",
      "Average rewards of the past 1000 episodes at iteration 5000: 94.90\n",
      "Average rewards of the past 1000 episodes at iteration 6000: 97.61\n",
      "Average rewards of the past 1000 episodes at iteration 7000: 97.22\n",
      "Average rewards of the past 1000 episodes at iteration 8000: 97.88\n",
      "Average rewards of the past 1000 episodes at iteration 9000: 96.62\n",
      "Average rewards of the past 1000 episodes at iteration 10000: 94.86\n",
      "Average rewards of the past 1000 episodes at iteration 11000: 97.71\n",
      "Average rewards of the past 1000 episodes at iteration 12000: 91.30\n",
      "Average rewards of the past 1000 episodes at iteration 13000: 84.72\n",
      "Average rewards of the past 1000 episodes at iteration 14000: 97.37\n",
      "Average rewards of the past 1000 episodes at iteration 15000: 95.39\n",
      "Average rewards of the past 1000 episodes at iteration 16000: 93.07\n",
      "Average rewards of the past 1000 episodes at iteration 17000: 92.74\n",
      "Average rewards of the past 1000 episodes at iteration 18000: 95.17\n",
      "Average rewards of the past 1000 episodes at iteration 19000: 93.91\n",
      "Average rewards of the past 1000 episodes at iteration 20000: 94.54\n",
      "WARNING: Average reward did not exceed average_reward_goal\n"
     ]
    }
   ],
   "source": [
    "_, _ = CartPole_Q_learning(eps=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $\\epsilon=0$, we in fact never learn and the average reward is stuck at about 9.3. \n",
    "\n",
    "When we used $\\epsilon=0.25$, the average rewards were much higher for the first few thousand episodes and exceeded 150 faster than when we used $\\epsilon=0.05$. This is because $\\epsilon=0.25$ allowed more exploration at the start, hence found the optimal policy more quickly.\n",
    "\n",
    "For $\\epsilon=0.5$, the average rewards increased quickly for the first few thousand episodes due to exploration. But it seems that the average rewards could not exceed 100 because it took random actions too often and terminated more easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.3. [10pts] Design your own experiment.** Design a modification that you think would either increase or reduce performance.  A simple example (which you can use) is initializing the Q-table differently, and thinking about how this might alter performance. Write down your idea, what you think might happen, and why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I personally think an adaptive $\\epsilon$ would be better, that is explore lots at the start and less later so I implimented that. In this problem, I also felt initilizing Q to random binary values would be more effective than 0 as going back and forth randomly is going to be a better policy for pole balancing than running left every time you don't know what to do. Finally, I changed this to double Q learning since we have stochastic dynamics and would like to avoid maximization bias. Here is the implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the experiment and report the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average rewards of the past 1000 episodes at iteration 1000: 33.62\n",
      "Average rewards of the past 1000 episodes at iteration 2000: 122.29\n",
      "Average rewards of the past 1000 episodes at iteration 3000: 240.47\n"
     ]
    }
   ],
   "source": [
    "eps = 1\n",
    "eps_decay = 0.999\n",
    "Q1 = np.random.randint(2,size=[num_bins]*len(obs)+[env.action_space.n]).astype('float32')\n",
    "Q2 = np.random.randint(2,size=[num_bins]*len(obs)+[env.action_space.n]).astype('float32')\n",
    "total_rewards = 0\n",
    "episode = 0\n",
    "\n",
    "# Outer Loop over episodes\n",
    "while True:\n",
    "    S = obs2bin(env.reset())\n",
    "    while True:            \n",
    "        A = np.argmax(Q1[S] + Q2[S]) if np.random.rand() > eps else np.random.choice([0,1])\n",
    "        S_prime, R, done, info = env.step(A)\n",
    "        S_prime = obs2bin(S_prime)\n",
    "\n",
    "        if np.random.rand() > 0.5:\n",
    "            Q1[S][A] = Q1[S][A] + alpha*(R + gamma*np.max(Q2[S_prime][np.argmax(Q1[S_prime])]) - Q1[S][A])\n",
    "        else:\n",
    "            Q2[S][A] = Q2[S][A] + alpha*(R + gamma*np.max(Q1[S_prime][np.argmax(Q2[S_prime])]) - Q2[S][A])\n",
    "        \n",
    "        total_rewards += R\n",
    "        S = S_prime\n",
    "\n",
    "        if done: break \n",
    "\n",
    "    if (episode + 1) % log_n == 0:\n",
    "        print(\"Average rewards of the past {} episodes at iteration {}: {:.2f}\".format(log_n, episode+1, total_rewards/log_n))\n",
    "        if total_rewards/log_n > 150: break\n",
    "        total_rewards = 0\n",
    "\n",
    "    if eps > 0.05:\n",
    "        eps *= eps_decay\n",
    "    episode += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach seemed quite effective, reaching the threshold in 3000-4000 episodes most of the times, which based on what I've seen was superior to the vanilla implementation. However, at times it can be as worse as the worst behaviour seen with the previous implementation. As for which change was actually most effective we would have to do some ablations, but I would guess the initilization would change the most."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## A. Extensions (optional)\n",
    "\n",
    "- does the learning rate make a difference?\n",
    "- visualize the Q-table to see which values are being updated and not\n",
    "- design a better binning strategy that uses fewer bins for a better-performing policy\n",
    "- extend this approach to work on different environments (e.g., LunarLander-v2)\n",
    "- extend this approach to work on environments with continuous actions, by using a fixed set of discrete samples of the action space.  e.g., for Pendulum-v0\n",
    "- implement a simple deep learning version of this.  we will see next homework that DQN uses some tricks to make the neural network training more stable.  Experiment directly with simply replacing the Q-table with a Q-Network and train the Q-Network using gradient descent with `loss = (targets - Q(s,a))**2`, where `targets = stop_grad(R + gamma * maxa(Q(s,a))`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3292922a618ca9f3a4e1cf088f6eaea80205f29535491ef474467c2b8876dcf3"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('python3.8': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
